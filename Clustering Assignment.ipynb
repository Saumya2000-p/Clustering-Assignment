{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "5227ce78-b8cc-4be5-b71e-46cffd830312",
      "cell_type": "code",
      "source": "                                                    ###Clustering Assignment###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "600cb947-d5a4-4c8f-a3c8-916b765963d7",
      "cell_type": "code",
      "source": "##Theoretical Questions:\n\n1. What is unsupervised learning in the context of machine learning?\n2. How does K-Means clustering algorithm work?\n3. Explain the concept of a dendrogram in hierarchical clustering.\n4. What is the main difference between K-Means and Hierarchical Clustering?\n5. What are the advantages of DBSCAN over K-Means?\n6. When would you use Silhouette Score in clustering?\n7. What are the limitations of Hierarchical Clustering?\n8. Why is feature scaling important in clustering algorithms like K-Means?\n9. How does DBSCAN identify noise points?\n10. Define inertia in the context of K-Means,\n11. What is the elbow method in K-Means clustering?\n12. Describe the concept of \"density in DBSCAN.\n13. Con hierarchical clustering be used on categorical data?\n14. What does a negative Silhouette Score indicate?\n15. Explain the term \"linkage criteria\" in hierarchical clustering.\n16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n17. What are the core parameters in DBSCAN, and how do they influence clustering?\n18. How does K-Means++ improve upon standard K-Means initialization?\n19. What is agglomerative clustering?\n20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n\n###Practical Questions:\n\n21. Generate synthetic data with 4 centers using make blobs and apply K-Means clustering. Visualize using a scatter plot\n22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n25. Use make_circles to generate synthetic data and cluster it using DBSCAN, Plot the result.\n26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN,\n28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n33. Generate synthetic data using make blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 20.\n43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n45. Generate synthetic data using make blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4cd67796-a02d-4de0-959d-f80c615fdbed",
      "cell_type": "code",
      "source": "Answer1:-Unsupervised learning is a type of machine learning where the algorithm learns patterns and relationships in the data without prior knowledge of the output or target variable.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6867e006-1ed9-44e9-bda2-3d8b57fbe7cc",
      "cell_type": "code",
      "source": "Answer2:-K-Means is a centroid-based clustering algorithm that partitions the data into K clusters based on the mean distance of the features.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "14cb3ef9-89ba-4b5b-ad46-8c93310bb975",
      "cell_type": "code",
      "source": "Answer3:-A dendrogram is a tree-like diagram that represents the hierarchical structure of the clusters in the data.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2bf15564-a5dd-40b5-8126-4264a6e01f58",
      "cell_type": "code",
      "source": "Answer4:-K-Means is a partitioning-based clustering algorithm, while Hierarchical Clustering is a hierarchical-based clustering algorithm.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c239c097-3eba-416b-b323-1b991906da38",
      "cell_type": "code",
      "source": "Answer5:-DBSCAN can handle varying densities and noise in the data, while K-Means is sensitive to outliers and requires a fixed number of clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "490868fc-9221-4fd8-8544-5761e0016177",
      "cell_type": "code",
      "source": "Answer6:-Silhouette Score is used to evaluate the quality of the clusters and determine the optimal number of clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e26f9ca9-b8db-4ad6-a24f-5e6ff87f60fd",
      "cell_type": "code",
      "source": "Answer7:-Hierarchical Clustering can be computationally expensive and sensitive to noise and outliers.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "37e52326-86e2-479b-a14b-80c03765027f",
      "cell_type": "code",
      "source": "Answer8:-Feature scaling is important to ensure that all features are on the same scale, which can improve the clustering results.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "596692c8-4d9d-4e0e-bf03-dfc2d0cfaeb8",
      "cell_type": "code",
      "source": "Answer9:-DBSCAN identifies noise points as points that do not belong to any cluster or have a density below a certain threshold.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b5b0f560-91f8-456e-bd93-01df2295b1b8",
      "cell_type": "code",
      "source": "Answer10:-Inertia is the sum of the squared distances between each point and its assigned centroid.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e8b1a330-f679-41b0-b5dc-e4d279389681",
      "cell_type": "code",
      "source": "Answer11:-The elbow method is a technique used to determine the optimal number of clusters by plotting the inertia against the number of clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0947664c-3388-4025-b7d5-d80f6279237f",
      "cell_type": "code",
      "source": "Answer12:-Density in DBSCAN refers to the number of points within a certain radius (epsilon) of a point.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7cd2750a-5f18-43f1-9568-e09bd0c1aff0",
      "cell_type": "code",
      "source": "Answer13:-Yes, hierarchical clustering can be used on categorical data, but it requires a suitable distance metric.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65e010a1-7768-448d-8dc8-41e275b2f1d7",
      "cell_type": "code",
      "source": "Answer14:-A negative Silhouette Score indicates that a point is assigned to the wrong cluster.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f2f4448-c557-4a3e-b12a-1c839bd23a88",
      "cell_type": "code",
      "source": "Answer15:-Linkage criteria refer to the method used to calculate the distance between clusters, such as single linkage, complete linkage, or average linkage.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74c09b5d-1a4d-47cb-a3f8-09c9bf848e66",
      "cell_type": "code",
      "source": "Answer16:-K-Means clustering can perform poorly on data with varying cluster sizes or densities because it is sensitive to outliers and requires a fixed number of clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a09bec6c-aadb-43c2-98a4-e4d3362b1ddb",
      "cell_type": "code",
      "source": "Answer17:-The core parameters in DBSCAN are epsilon (radius) and min_samples (minimum number of points). These parameters influence the clustering results by controlling the density and noise tolerance.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2cd0a522-f347-4f86-a50c-b98fe4983fd3",
      "cell_type": "code",
      "source": "Answer18:-K-Means++ improves upon standard K-Means initialization by selecting the initial centroids in a way that spreads them out more evenly.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a5b58844-4e55-4546-8cce-47a0793a4a3a",
      "cell_type": "code",
      "source": "Answer19:-Agglomerative clustering is a type of hierarchical clustering that starts with each point as a separate cluster and merges them into larger clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aa12de20-40cc-403d-a315-78acf15d6c74",
      "cell_type": "code",
      "source": "Answer20:-Silhouette Score is a better metric than inertia because it takes into account both the cohesion and separation of the clusters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5fabdff3-e98a-44b8-ad1a-870dd89e6348",
      "cell_type": "code",
      "source": "Answer21:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=4, cluster_std=0.60, random_state=0)\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4acbe9f9-1b17-4fea-8a2b-1d54360d40ca",
      "cell_type": "code",
      "source": "Answer22:-from sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\n\niris = load_iris()\nX = iris.data\n\nagglo = AgglomerativeClustering(n_clusters=3)\nagglo.fit(X)\nprint(agglo.labels_[:10])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5f61a5d9-5968-40f4-8e9c-c69bc8c1f259",
      "cell_type": "code",
      "source": "Answer23:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, noise=0.05)\ndbscan = DBSCAN(eps=0.3, min_samples=10)\ndbscan.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9012c3a-c670-45d2-be89-d045fa8636b4",
      "cell_type": "code",
      "source": "Answer24:-from sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nwine = load_wine()\nX = wine.data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X_scaled)\nprint(np.bincount(kmeans.labels_))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b73a1ac4-73d9-4bee-ac85-d5bda355f72f",
      "cell_type": "code",
      "source": "Answer25:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=200, noise=0.05, factor=0.5)\ndbscan = DBSCAN(eps=0.3, min_samples=10)\ndbscan.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_)\nplt.show()\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35056b20-d601-44eb-8217-ab0f336f4f88",
      "cell_type": "code",
      "source": "Answer26:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X_scaled)\nprint(kmeans.cluster_centers_)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ea0cfea8-7bac-415f-b8f5-96d0e769dfa8",
      "cell_type": "code",
      "source": "Answer27:-import numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3, cluster_std=[0.5, 1, 1.5])\ndbscan = DBSCAN(eps=1, min_samples=10)\ndbscan.fit(X)\nprint(dbscan.labels_)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b42f14d0-df95-48cb-b88b-b5c794e2e672",
      "cell_type": "code",
      "source": "Answer28:-from sklearn.datasets import load_digits\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndigits = load_digits()\nX = digits.data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nkmeans = KMeans(n_clusters=10)\nkmeans.fit(X_pca)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "304bf45d-f740-46e6-91d8-98f84aadcefe",
      "cell_type": "code",
      "source": "Answer29:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3)\n\nsilhouette_scores = []\nfor k in range(2, 6):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n\nplt.bar(range(2, 6), silhouette_scores)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e8e0a1a2-8936-4369-9626-05d2e747fd58",
      "cell_type": "code",
      "source": "Answer30:-from sklearn.datasets import load_iris\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\niris = load_iris()\nX = iris.data\nZ = linkage(X, method='average')\ndendrogram(Z)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97d90786-ecca-4246-8e56-ab400a3f9f45",
      "cell_type": "code",
      "source": "Answer31:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3, cluster_std=1.5)\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.4)\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65bca9af-da1e-4a00-a7a4-93ddebc55b2c",
      "cell_type": "code",
      "source": "Answer32:-from sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\n\ndigits = load_digits()\nX = digits.data\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\n\ndbscan = DBSCAN(eps=0.5, min_samples=10)\ndbscan.fit(X_tsne)\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "26deda08-0ae8-4c9a-be6e-43f0f87ba849",
      "cell_type": "code",
      "source": "Answer33:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3)\nagglo = AgglomerativeClustering(n_clusters=3, linkage='complete')\nagglo.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=agglo.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "130a4c7c-d39c-4ee3-b204-19a20f5ecc04",
      "cell_type": "code",
      "source": "Answer34:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\n\ninertia_values = []\nfor k in range(2, 7):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    inertia_values.append(kmeans.inertia_)\n\nplt.plot(range(2, 7), inertia_values)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "95c3f10c-9587-4f7d-b8de-1fa095e29e5c",
      "cell_type": "code",
      "source": "Answer35:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=200, noise=0.05, factor=0.5)\nagglo = AgglomerativeClustering(n_clusters=2, linkage='single')\nagglo.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=agglo.labels_)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "72a08db2-c4b2-4893-a7d3-933863ce7623",
      "cell_type": "code",
      "source": "Answer36:-from sklearn.datasets import load_wine\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nwine = load_wine()\nX = wine.data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndbscan = DBSCAN(eps=0.5, min_samples=10)\ndbscan.fit(X_scaled)\nn_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\nprint(n_clusters)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "112d4e70-526b-4359-854b-eb89af8b970e",
      "cell_type": "code",
      "source": "Answer37:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3)\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=200, c='red')\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f6441ec0-c992-47fc-b88a-5a99780889fe",
      "cell_type": "code",
      "source": "Answer38:-from sklearn.datasets import load_iris\nfrom sklearn.cluster import DBSCAN\n\niris = load_iris()\nX = iris.data\ndbscan = DBSCAN(eps=0.5, min_samples=10)\ndbscan.fit(X)\nnoise_samples = np.sum(dbscan.labels_ == -1)\nprint(noise_samples)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33d37ae4-1e49-4170-ae93-7002cff028df",
      "cell_type": "code",
      "source": "Answer39:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, noise=0.05)\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "10d19cc5-a829-4c2c-8bbf-0b01a8f98135",
      "cell_type": "code",
      "source": "Answer40:-from sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndigits = load_digits()\nX = digits.data\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X)\n\nkmeans = KMeans(n_clusters=10)\nkmeans.fit(X_pca)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=kmeans.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4baa9df7-cf0a-4aea-b6cf-b7812394ab92",
      "cell_type": "code",
      "source": "Answer41:-import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=5)\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X)\nsilhouette = silhouette_score(X, kmeans.labels_)\nprint(silhouette)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0cec8ef0-7071-4eb1-831a-a3c570212478",
      "cell_type": "code",
      "source": "Answer42:-from sklearn.datasets import load_breast_cancer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nagglo = AgglomerativeClustering(n_clusters=2)\nagglo.fit(X_pca)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=agglo.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b18b01dd-89ea-4f88-853e-51e6af361943",
      "cell_type": "code",
      "source": "Answer43:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=200, noise=0.05, factor=0.5)\n\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\n\ndbscan = DBSCAN(eps=0.3, min_samples=10)\ndbscan.fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\nax[1].scatter(X[:, 0], X[:, 1], c=dbscan.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a6a986de-d096-452c-bf56-c706439e322b",
      "cell_type": "code",
      "source": "Answer44:-from sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples\nimport matplotlib.pyplot as plt\n\niris = load_iris()\nX = iris.data\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\nsilhouette_values = silhouette_samples(X, kmeans.labels_)\nplt.bar(range(len(silhouette_values)), silhouette_values)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7827a1ac-2f85-4152-9ad0-c94e9fbf4674",
      "cell_type": "code",
      "source": "Answer45:-import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=3)\nagglo = AgglomerativeClustering(n_clusters=3, linkage='average')\nagglo.fit(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=agglo.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0686071a-822f-4732-a2ff-958516de607e",
      "cell_type": "code",
      "source": "Answer46:-import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\n\nwine = load_wine()\nX = wine.data[:, :4]\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\ndf = pd.DataFrame(X, columns=wine.feature_names[:4])\ndf['cluster'] = kmeans.labels_\n\nsns.pairplot(df, hue='cluster')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5927e6eb-59eb-4b13-ba02-8909a38f965a",
      "cell_type": "code",
      "source": "Answer47:-import numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=200, centers=2, cluster_std=0.8, random_state=0)\ndbscan = DBSCAN(eps=0.5, min_samples=10)\ndbscan.fit(X)\n\nn_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\nn_noise = list(dbscan.labels_).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters)\nprint(\"Estimated number of noise points: %d\" % n_noise)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c468168a-35a7-49be-a553-021f33ba41f3",
      "cell_type": "code",
      "source": "Answer48:-from sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\n\ndigits = load_digits()\nX = digits.data\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\n\nagglo = AgglomerativeClustering(n_clusters=10)\nagglo.fit(X_tsne)\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=agglo.labels_)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}